%%% Материалы и методы %%%
\section{ТЕОРЕТИЧЕСКИЕ ОСНОВЫ СТАТИСТИЧЕСКОГО АНАЛИЗА И МАШИННОГО ОБУЧЕНИЯ}

\subsection{Коэффициенты корреляции}

Коэффициенты корреляции используются для измерения силы и направления связи между двумя переменными. В зависимости от типа данных и целей исследования применяются различные коэффициенты: коэффициент Пирсона — для линейной связи между количественными переменными, коэффициенты Спирмена и Кендалла — для ранговой корреляции, устойчивой к выбросам и нелинейностям.

\subsubsection{Корреляция Кендалла-тау}

Коэффициент $\tau$ Кендалла измеряет степень согласованности пар наблюдений. Он основан на подсчёте числа согласованных ($C$) и несогласованных ($D$) пар:

\[
\tau = \frac{C - D}{\frac{1}{2}n(n-1)}
\]

где $n$ — общее число наблюдений. Метод обладает высокой устойчивостью к выбросам и предпочтителен при наличии нечисловых шкал или неполной информации о распределении данных.

Коэффициент Кендалла был предложен в работе М. Кендалла в 1938 году \cite{kenda1938}.

\subsubsection{Корреляция Пирсона}

Коэффициент корреляции Пирсона $r$ — наиболее распространённая мера линейной зависимости между двумя количественными переменными:

\[
r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2 \sum_{i=1}^{n}(y_i - \bar{y})^2}}
\]

где $\bar{x}$ и $\bar{y}$ — средние значения переменных $x$ и $y$. Значение $r \in [-1, 1]$, где $\pm1$ указывает на идеальную линейную зависимость. При этом $r = 0$ не означает отсутствие связи, а лишь отсутствие линейной зависимости.

Метод предложен К. Пирсоном в 1896 году \cite{pearson1896}.

\subsubsection{Корреляция Спирмена}

Коэффициент Спирмена $\rho$ оценивает монотонную зависимость между переменными и определяется как коэффициент Пирсона между ранжированными значениями:

\[
\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}
\]

где $d_i$ — разность между рангами $x_i$ и $y_i$ в выборке. Подходит для оценки силы связи в случае нелинейных, но монотонных зависимостей.

Впервые описан Ч. Спирменом в 1904 году \cite{spearman1904}.

\subsection{Примеры вычисления коэффициентов корреляции}

Рассмотрим выборку из пяти наблюдений:

\[
X = \{2, 4, 6, 8, 10\}, \quad Y = \{1, 3, 4, 7, 9\}
\]

\textbf{Пирсон:} $\bar{X} = 6$, $\bar{Y} = 4.8$

\[
r = \frac{\sum (X_i - 6)(Y_i - 4.8)}{\sqrt{\sum (X_i - 6)^2 \cdot \sum (Y_i - 4.8)^2}} = \frac{40}{\sqrt{40 \cdot 40.8}} \approx 0.99
\]

\textbf{Спирмен:} Ранги $X = \{1,2,3,4,5\}$, $Y = \{1,2,3,4,5\}$, $\rho = 1$

\textbf{Кендалл:} Все пары согласованные $\Rightarrow \tau = 1$

Таким образом, все коэффициенты показывают высокую положительную корреляцию.


\subsection{Статистические критерии проверки значимости коэффициентов корреляции}

Для оценки статистической значимости коэффициентов корреляции применяются специализированные критерии, зависящие от типа корреляционной меры и распределения данных. Рассмотрим основные подходы, применяемые на практике.

\subsubsection{t-критерий для коэффициента корреляции Пирсона}

Для проверки гипотезы $H_0: \rho = 0$ (отсутствие линейной связи) используется t-распределение:

\[
t = r \cdot \sqrt{\frac{n - 2}{1 - r^2}}
\]

где $r$ — выборочный коэффициент корреляции Пирсона, $n$ — размер выборки. Статистика $t$ подчиняется t-распределению с $n - 2$ степенями свободы. Полученное значение сравнивается с критическим, соответствующим заданному уровню значимости $\alpha$.

Метод используется при соблюдении условий нормальности и независимости наблюдений \cite{bonett2000}.

\subsubsection{z-преобразование Фишера для корреляций}

Для оценки значимости разности между двумя независимыми коэффициентами корреляции или построения доверительного интервала для одного $r$ применяется преобразование Фишера:

\[
z = \frac{1}{2} \ln\left(\frac{1 + r}{1 - r}\right)
\]

где $z$ асимптотически нормально распределён с дисперсией $1 / (n - 3)$. Это позволяет использовать стандартные нормальные границы при проверке гипотез:

\[
Z = \frac{z - z_0}{\sqrt{1 / (n - 3)}}
\]

Метод применяется как при сравнении корреляций между группами, так и при проверке отличия $r$ от заданного значения \cite{fisher1921}.

\subsubsection{Критерии значимости для корреляций Кендалла и Спирмена}

Коэффициенты Спирмена $\rho$ и Кендалла $\tau$ требуют иных подходов. При достаточно большой выборке (обычно $n > 10$) их значения стандартизируются, и используется аппроксимация нормальным распределением:

Для Спирмена:
\[
Z = \frac{\rho \cdot \sqrt{n - 1}}{\sqrt{1 - \rho^2}}
\]

Для Кендалла:
\[
Z = \frac{3 \cdot \tau \cdot \sqrt{n(n - 1)}}{\sqrt{2(2n + 5)}}
\]

Значения $Z$ сравниваются с критическим значением нормального распределения. Альтернативно можно использовать точные p-value при малых $n$ (например, через перестановочные тесты) \cite{zimmerman1997}.

\subsubsection{Примеры применения критериев для проверки значимости коэффициентов корреляции}

Рассмотрим пример применения t-критерия для коэффициента Пирсона. Для выборки из $n = 20$ пар наблюдений получено $r = 0.57$. Тогда

\[
t = 0.57 \cdot \sqrt{\frac{20 - 2}{1 - 0.57^2}} \approx 3.03
\]

Критическое значение $t_{crit}$ для $\alpha = 0.05$, $df = 18$ составляет 2.101. Так как $3.03 > 2.101$, нулевая гипотеза отклоняется — корреляция статистически значима.

Для сравнения двух коэффициентов корреляции с использованием z-преобразования Фишера:

\[
r_1 = 0.65, \quad r_2 = 0.30, \quad n_1 = n_2 = 50
\]
\[
z_1 = 0.5 \ln \left(\frac{1 + 0.65}{1 - 0.65}\right) \approx 0.775, \quad z_2 \approx 0.309
\]
\[
Z = \frac{0.775 - 0.309}{\sqrt{1/(50 - 3) + 1/(50 - 3)}} \approx 3.72
\]

При $Z > 1.96$, разница между коэффициентами статистически значима на уровне 0.05.

\subsection{Доверительные интервалы для доли (бернуллиевой величины)}

Доверительный интервал (ДИ) для доли — это интервал, в котором с заданной вероятностью содержится истинное значение параметра $p$ распределения Бернулли. Он широко применяется в задачах бинарной классификации, в анализе успехов/неудач, а также в A/B тестировании.

\subsubsection{Классические подходы построения интервалов}

\paragraph{Wilson score interval}

Wilson-интервал улучшает симметричность классического нормального приближения. Он основан на перестроенной статистике и формулируется следующим образом:

\[
\hat{p}_W = \frac{\hat{p} + \frac{z^2}{2n}}{1 + \frac{z^2}{n}}, \quad
ME = \frac{z}{1 + \frac{z^2}{n}} \cdot \sqrt{\frac{\hat{p}(1 - \hat{p})}{n} + \frac{z^2}{4n^2}}
\]

\[
CI = \hat{p}_W \pm ME
\]

где $z$ — квантиль стандартного нормального распределения. Этот метод особенно точен при малых $n$ и при $\hat{p}$, близком к 0 или 1 \cite{brown2001interval}.

\paragraph{Jeffreys interval}

Интервал Джеффриса основан на байесовском подходе с априорным распределением Бета(0.5, 0.5). Доверительный интервал строится как:

\[
CI = \text{Beta}^{-1}_{\alpha/2}(x + 0.5, n - x + 0.5)
\]

где $x$ — число успехов. Интервал обладает хорошими частотными свойствами и симметрией \cite{brown2001interval}.

\paragraph{Clopper–Pearson interval}

Точный Clopper–Pearson интервал строится на основе биномиального распределения:

\[
CI = [B(\alpha/2; x, n - x + 1), B(1 - \alpha/2; x + 1, n - x)]
\]

где $B$ — обратная функция неполной бета-функции. Несмотря на точность, интервал часто критикуется за избыточную ширину \cite{clopper1934use}.

\paragraph{Agresti–Coull interval}

Agresti–Coull-интервал — это аппроксимация Wilson-интервала, предлагающая удобное приближение:

\[
\tilde{n} = n + z^2, \quad \tilde{x} = x + \frac{z^2}{2}, \quad \tilde{p} = \frac{\tilde{x}}{\tilde{n}}, \quad
CI = \tilde{p} \pm z \cdot \sqrt{\frac{\tilde{p}(1 - \tilde{p})}{\tilde{n}}}
\]

Он используется благодаря простоте вычислений и приемлемой точности \cite{agresti1998}.

\subsubsection{Сравнение и обсуждение доверительных интервалов}

Сравнение различных подходов показывает, что нормальное приближение (так называемый Wald-интервал) часто имеет заниженную фактическую надёжность, особенно при малых объёмах выборки ($n < 30$) или при оценках вероятности, близких к $0$ или $1$. Поэтому он не рекомендуется для практического применения.

Wilson и Jeffreys интервалы демонстрируют лучшие частотные свойства и надёжность. Они особенно полезны в ситуациях, когда выборка небольшая или доля успехов близка к краевым значениям. Wilson-интервал рекомендуется при необходимости симметричной аппроксимации и строгом контроле ширины интервала, в том числе в A/B тестировании и биостатистике. Jeffreys-интервал актуален в байесовском контексте и показывает стабильные частотные свойства даже при очень малых $n$.

Clopper–Pearson интервал, будучи «точным», часто даёт чрезмерно широкие границы, особенно при небольшом числе наблюдений. Он подходит, когда важна консервативность, например, в фармацевтических испытаниях или регулированной среде, где требуется минимизировать вероятность ложноположительных решений.

Agresti–Coull-интервал — хорошая альтернатива для быстрой оценки: он не требует сложных вычислений и обеспечивает удовлетворительную точность. Рекомендуется для практических приложений, когда важен баланс между вычислительной простотой и приемлемой статистической точностью.

Таким образом, выбор метода зависит от контекста:

\begin{itemize}
    \item \textbf{Wilson:} рекомендуется как надёжный метод общего назначения;
    \item \textbf{Jeffreys:} подходит для байесовских и частотных оценок при малых $n$;
    \item \textbf{Clopper–Pearson:} применяется при необходимости строгости и регуляторных ограничений;
    \item \textbf{Agresti–Coull:} используется в прикладных задачах при больших $n$;
    \item \textbf{Wald:} не рекомендуется (только в случае $n > 100$ и $p \approx 0.5$).
\end{itemize}

Сравнительный анализ этих подходов можно найти в обобщающем исследовании Brown et al. (2001) \cite{brown2001interval}.


\subsubsection{Примеры вычисления доверительных интервалов}

Пусть в эксперименте из $n = 100$ наблюдений получено $x = 20$ успехов ($\hat{p} = 0.2$), при уровне доверия 95\%:

\begin{itemize}
    \item \textbf{Wilson:} $CI \approx [0.129, 0.296]$
    \item \textbf{Jeffreys:} $CI \approx [0.129, 0.298]$
    \item \textbf{Clopper–Pearson:} $CI \approx [0.127, 0.299]$
    \item \textbf{Agresti–Coull:} $CI \approx [0.133, 0.300]$
\end{itemize}

Эти результаты демонстрируют, что при небольших выборках все четыре метода дают схожие оценки, но Wilson и Jeffreys обеспечивают лучшую симметрию и сдержанность по ширине интервала.

\subsection{Методы и модели машинного обучения}
\subsubsection{Логистическая регрессия: теоретические аспекты и интерпретация результатов}
\subsubsection{Модель градиентного бустинга CatBoost}
\subsubsection{Интерпретация моделей машинного обучения с помощью SHAP}
\clearpage